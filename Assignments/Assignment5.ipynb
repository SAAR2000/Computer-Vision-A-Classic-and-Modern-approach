{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CV_Stamatics_A5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvFM645NE-D2"
      },
      "source": [
        "# Assignment 5\n",
        "In this assignment, we will go through Perceptron, Linear Classifiers, Loss Functions, Gradient Descent, Back Propagation, Neural Networks and Convulutional Neural Networks.\n",
        "\n",
        "\n",
        "PS. this one is not from Stanford's course.\n",
        "\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "## Instructions\n",
        "* This notebook contain blocks of code, you are required to complete those blocks(where required)\n",
        "* You are required to copy this notebook (\"copy to drive\" above) and complete the code.\n",
        "* For Submission, You'll be required to submit a sharable link for your copy of this notebook. (DO NOT CHANGE THE NAME OF THE FUNCTIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "QLtp15rqE-EU"
      },
      "source": [
        "# Part 1: Perceptron\n",
        "In this section, we will see how to implement a perceptron. Goal would be for you to delve into the mathematics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zao4e-DphaGA"
      },
      "source": [
        "## Intro\n",
        "What's a perceptron? It's an algorithm modelled on biological computational model to classify things into binary classes. It's a supervides learning algorithm, meaning that you need to provide labelled data containing features and the actual classifications. A perceptron would take these features as input and spit out a binary value (0 or 1). While training the model with training data, we try to minimise the error and learn the parameters involved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDTUoAd6ixm-"
      },
      "source": [
        "**How does it work?**\\\n",
        "A perceptron is modelled on a biological neuron. A neuron has input dendrites and the output is carried by axons. Similarly, a perceptron takes inputs called \"features\". After processing, a perceptron gives output. For computation, it has a \"weight\" vector which is multipled with feature vector. An activation function is added to introduce some non linearities and the output is given out.\\\n",
        "It can be represented as: $$  f=\\sum_{i=1}^{m} w_ix_i +b$$\n",
        "\n",
        "Let's implement this simple function to give an output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXezofBIgzId"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class perceptron():\n",
        "    def __init__(self,num_input_features=8):\n",
        "        self.weights = np.random.randn(num_input_features)\n",
        "        self.bias = np.random.random()\n",
        "\n",
        "    def activation(self,x):\n",
        "        '''\n",
        "            Implement heavside step activation function here (google ;))\n",
        "        '''\n",
        "        pass\n",
        "        return (x>0 and 1) or 0\n",
        "\n",
        "    def forward(self,x: np.ndarray):\n",
        "        '''\n",
        "            you have random initialized weights and bias\n",
        "            you can access then using `self.weights` and `self.bias`\n",
        "            you should use activation function before returning\n",
        "        \n",
        "            x : input features\n",
        "            return : a binary value as the output of the perceptron \n",
        "        '''\n",
        "        # YOUR CODE HERE\n",
        "        p = np.dot(x.T, self.weights) + self.bias\n",
        "        act = self.activation(p)\n",
        "        pass\n",
        "        # YOUR CODE HERE\n",
        "        return act"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKwDFAyocVo"
      },
      "source": [
        "np.random.seed(0)\n",
        "perc = perceptron(8)\n",
        "assert perc.forward(np.arange(8))==1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NWTTg1e9r7uM"
      },
      "source": [
        "# Part 2: Linear Classifier\n",
        "In this section, we will see how to implement a linear Classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYDO4GcHr7uM"
      },
      "source": [
        "## Intro\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HFvjH06r7uN"
      },
      "source": [
        "**How does it work?**\n",
        "\n",
        "Linear Classifier uses the following function: $$Y = WX+b$$ Where, $W$ is a 2d array of weights with shape (#features, #classes).\n",
        "\n",
        "\n",
        "Let's implement this classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A13CEkGr7uN"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearClassifier():\n",
        "    def __init__(self,num_input_features=32,num_classes=5):\n",
        "        self.weights = np.random.randn(num_input_features,num_classes)\n",
        "        self.bias = np.random.rand(num_classes)\n",
        "\n",
        "    def forward(self,x: np.ndarray):\n",
        "        '''\n",
        "            x: input features\n",
        "            you have random initialized weights and bias\n",
        "            you can access then using `self.weights` and `self.bias`\n",
        "            return an output vector of num_classes size\n",
        "        '''\n",
        "        # YOUR CODE HERE\n",
        "        Y = np.dot(x, self.weights) + self.bias\n",
        "        pass\n",
        "        # YOUR CODE HERE\n",
        "        return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgzPxyTsr7uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499c7309-f9c5-4fad-befb-efbb4dd8be2e"
      },
      "source": [
        "np.random.seed(0)\n",
        "lc = LinearClassifier()\n",
        "lc.forward(np.random.rand(1,32))\n",
        "# Should be close to:\n",
        "# array([[ 1.30208164,  5.58136003,  0.87793013, -4.7332119 ,  4.81172123]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.30208164,  5.58136003,  0.87793013, -4.7332119 ,  4.81172123]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ZVgOVzJetuqo"
      },
      "source": [
        "# Part 3: Loss Functions, Gradient descent and Backpropagation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pXryjpctuqy"
      },
      "source": [
        "## Intro\n",
        "\n",
        "Loss Functions tells how \"off\" the output od our model is. Based upon the application, you can use several different loss functions. Formally, A loss function is a function $L:(z,y)\\in\\mathbb{R}\\times Y\\longmapsto L(z,y)\\in\\mathbb{R}$ that takes as inputs the predicted value $z$ corresponding to the real data value yy and outputs how different they are We'll implement L1 loss, L2 loss, Logistic loss, hinge loss and cross entropy loss functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGRb8BHotuqy"
      },
      "source": [
        "### **L1 loss**\n",
        "L1 loss is the linear loss function  $L = \\dfrac{1}{2}(y−z) $\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxVh6IL2tuqz"
      },
      "source": [
        "import numpy as np\n",
        "def L1Loss(z,y):\n",
        "    '''\n",
        "        y : True output.\n",
        "        z : Predicted output.\n",
        "        return : L\n",
        "    '''\n",
        "    L = (1/2)*(y-z)\n",
        "    pass\n",
        "    return L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xy8ZS84cKtQ"
      },
      "source": [
        "### **L2 loss**\n",
        "L2 loss is the quadratic loss function or the least square error function  $L = \\dfrac{1}{2}(y−z)^2 $\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JThp5P-KcKtS"
      },
      "source": [
        "import numpy as np\n",
        "def L2Loss(z,y):\n",
        "    '''\n",
        "        y : True output. \n",
        "        z : Predicted output. \n",
        "        return : L\n",
        "    '''\n",
        "    L = (1/2)*np.power(y-z,2)\n",
        "    pass\n",
        "    return L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2JNLnWYcLSC"
      },
      "source": [
        "### **Hinge Loss**\n",
        "Hinge loss is: $ L = max( 0, 1 - yz ) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ1YM4J-cLSC"
      },
      "source": [
        "import numpy as np\n",
        "def hingeLoss(z,y):\n",
        "    '''\n",
        "        y : True output. \n",
        "        z : Predicted output. \n",
        "        return : L\n",
        "    '''\n",
        "    L = np.maximum(0, 1-y*z)\n",
        "    pass\n",
        "    return L\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m15_MjradMNY"
      },
      "source": [
        "### **Cross Entropy Loss**\n",
        "Another very famous loss function is Cross Entropy loss: $ L = −[ylog(z)+(1−y)log(1−z)] $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snJLqhszdMNY"
      },
      "source": [
        "import numpy as np\n",
        "def CELoss(z,y):\n",
        "    '''\n",
        "        y : True output. \n",
        "        z : Predicted output. \n",
        "        return : L\n",
        "    '''\n",
        "    L = -1*(y*np.log(z) + (1-y)*np.log(1-z))\n",
        "    pass\n",
        "    return L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsRPsfzxyEVL"
      },
      "source": [
        "### **0-1 Loss**\n",
        "Loss Function used by perceptron is: $ \\begin{cases} \n",
        "      0=z-y & z=y \\\\\n",
        "      1=\\dfrac{z-y}{z-y} & z\\neq y\n",
        "   \\end{cases} $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sA7GxLHyEVM"
      },
      "source": [
        "import numpy as np\n",
        "def zeroOneLoss(z,y):\n",
        "    '''\n",
        "        y : True output. \n",
        "        z : Predicted output. \n",
        "        return : L\n",
        "    '''\n",
        "    L = np.zeros(y.shape)\n",
        "    x = z-y\n",
        "    L[x[:,0] != 0] = 1\n",
        "    pass\n",
        "    return L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWhbibHcgRR8"
      },
      "source": [
        "## Cost Function\n",
        "The cost function $J$ is commonly used to assess the performance of a model, and is defined with the loss function $L$ as follows:\n",
        "$$\\boxed{J(\\theta)=\\sum_{i=1}^mL(h_\\theta(x^{(i)}), y^{(i)})}$$\n",
        "where $h_\\theta$ is the hypothesis function i.e. the function used to predict the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSbmhW4og97t"
      },
      "source": [
        "lossFunctions = {\n",
        "    \"l1\" : L1Loss,\n",
        "    \"l2\" : L2Loss,\n",
        "    \"hinge\" : hingeLoss,\n",
        "    \"cross-entropy\" : CELoss,\n",
        "    \"0-1\" : zeroOneLoss\n",
        "}\n",
        "\n",
        "def cost(Z : np.ndarray, Y : np.ndarray, loss : str):\n",
        "    '''\n",
        "        Z : a numpy array of predictions.\n",
        "        Y : a numpy array of true values.\n",
        "        return : A numpy array of costs calculated for each example.\n",
        "    '''\n",
        "    loss_func = lossFunctions[loss]\n",
        "    # YOUR CODE HERE\n",
        "    J = loss_func(z, y)\n",
        "    # YOUR CODE HERE\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upsN7A0zjGqx"
      },
      "source": [
        "## Gradient Descent and Back Propagation\n",
        "Gradient Descent is an algorithm that minimizes the loss function by calculating it's gradient. By noting $\\alpha\\in\\mathbb{R}$ the learning rate, the update rule for gradient descent is expressed with the learning rate $\\alpha$ and the cost function $J$ as follows:\n",
        "\n",
        "$$\\boxed{ W \\longleftarrow W -\\alpha\\nabla J( W )}$$\n",
        "​\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFCN-fYCqidi"
      },
      "source": [
        "But we need to find the partial derivative of Loss function wrt every parameter to know what is the slight change that we need to apply to our parameters. This becomes particularly hard if we have more than 1 layer in our algorithm. Here's where **Back Propagation** comes in. It's a way to find gradients wrt every parameter using the chain rule. Backpropagation is a method to update the weights in the neural network by taking into account the actual output and the desired output. The derivative with respect to weight ww is computed using chain rule and is of the following form:\n",
        "\n",
        "$$\\boxed{\\frac{\\partial L(z,y)}{\\partial w}=\\frac{\\partial L(z,y)}{\\partial a}\\times\\frac{\\partial a}{\\partial z}\\times\\frac{\\partial z}{\\partial w}}$$\n",
        "​\n",
        " \n",
        "As a result, the weight is updated as follows:\n",
        "\n",
        "$$\\boxed{w\\longleftarrow w-\\alpha\\frac{\\partial L(z,y)}{\\partial w}}$$\n",
        "\n",
        "So, In a neural network, weights are updated as follows:\n",
        "\n",
        "* Step 1: Take a batch of training data.\n",
        "* Step 2: Perform forward propagation to obtain the corresponding loss.\n",
        "* Step 3: Backpropagate the loss to get the gradients.\n",
        "* Step 4: Use the gradients to update the weights of the network.\n",
        "​\n",
        " \n",
        "Now, Assuming that you know Back Propagation (read a bit about it, if you don't), we'll now implement a 3 layer digit classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyplk5PLEUsJ"
      },
      "source": [
        "# Get MNIST dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X = (x/255).astype('float32')\n",
        "Y = to_categorical(y)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.15, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "qQhkATYhEkkC",
        "outputId": "ef3dac11-90e2-46ac-ace0-91884cc82225"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "# Let's display 1 image from this data set\n",
        "index = np.random.choice(x.shape[1])\n",
        "test_img = cv2.resize(x[index].reshape(28,28),(256,256))\n",
        "test_label = y[index]\n",
        "cv2_imshow(test_img)\n",
        "print(\"label: \",test_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAg0UlEQVR4nO1963ojt44teGdVSXJnz/u/4TlJt6Wq4hWYHyBL8qU7e4+kjjyjJcdf8sWSizBAgrgsADzxxBNPPPHEE0888cQTTzzxxBNPPPHEE0888cQTTzzxxBNPPPHEE//bIf7pB2gQAIJf7ds7EAARf1H775v94geAEEIIKYSQUkoppJQCoD0ctS/CiohYEbsobvKr9U0+5WrwyqVUSiullFaiLV8Q9D98KZW/qOEmv/kxBCCkUlIppbQ2WhtttAQBLAMiXj+mnHPOSQgkJAQSN5HAgwhASKW00tpYY62x1sq+KwDynxtriilGJQAQBQLeZv2PIgAplTZaG+saVN8TiZAICamEELSSQFSFACDxv0kDQCqltTXGe+8H7/2g+oEAiIRISHldtBKAtYgKRPJG58BjCEAIqbQx1vphHMZhHAcNgiVAiISIiNlqJYFqyRWAiG50fj2IAKRU2lhn/ThN4zRNoxFNAoB8/GHiv39OWgApwo++wv8I/7AA+LQT2ljnvfN+nHbTNE3TTgsBomkASyAJAUCESDmrLIBkvcUT/KMCEEIKIYUUfhgGPwzDMDa8MQG2AY0gpNLG+hRTTFECAjU36Qr8swKQSioplTwvvEtCifMpwLug6esfwhrWIAAre4TXSeCfFoDSSmk1dXjvnffeedXWD4DEr9LXP8xuURKwSOLlX3Ue/rMCUHz663Ha7Xa7/W7nrbPOWWfVW0cIiYpQ2li/DqOzWgCWpAiJCL+2BmhjrDHjtDvs9/vD3hlrrLHGKOi+cPf8q9LGujiE1WgJWJKRBIQgvrYAjLHO2nG3378cDocXazRfB8R2HeQbMFHVxjmfYlq1BMwpaIVAV7vE/7QAtLHO+XHaHQ7fXl6+WaW222C/qhO/0Nqccko5SMCS4mqUABRCXOcTP4IA/DBN+/3Lyx/f/jDtWizfeDkEAIDZ5VJyyYGwpLBYLYEArvUI/wkBiB4AMsZa5/0wjNO02x8OLy9GdHx4F5pSSy21xJzCMnhrdAWAa33i3yoAAQJAbMEPyfce74dpHLw1WsnPVn5+u5CkSCmttTHGWluLLECE1zzT7xSAYOdOaq201lpr5533zjs/jd5Zw+v/9M/P7xaSSInalm9dEQKA8CoV+I0C6MotjbHWWGOts84565ybBtaAJqKfRCqFJAkgugI4J4UAQvlVBACCHX9lnHfOO+dtxzQOrpkA28nP3g4A4mwCAgixXnct/J0aAEJKIaUyzg/DMA6DMcYaY6yZRu+c0bwF/EICEoSQ2JbvHBAhSnnVU/1mDRBSSmXc0C79ur3GbROEnxoACJAoSdCmAJYIa5VfRwN4+VIb58f9br/bad4OlR68d9ZotoCffwBIAiLcNgHCWov6WnuAlEoZN4y7w+Flrzqc984a9eu1CAABBESbCbT1fxEB9OVrbd0w7Q7fXl6UlBwPsM46PgX4Zz9L+5xtYzOBWotW8tE1oAf3lDZGG6ONO7wcXg67aRy63yut0UpJCc3x7+snuMgYXnyikEppY2zJV6//dwigBb6UddZaZ63fH/b7w34aB84CSimNaWfglgdpGkDNe5CXe4OQUmltrC1ZfwEN6Hk/7RuG3bTb7XbT6KUU/NKGVyKIOAS2RbpISCmkgEsJbBqQjVYPvwcIIZVUSirjx5Gj/hPHvseBVUMIqbRWSvWwL2LlMA8Bi4/U2yuPlFJrbW26XgF+hwlIqbRSyg4jR7zHcRj4a7v6KaUUL4UQK1asPSkOQkn1zjfaNMCYr2AC/LhaOz9M+/1uv991W/B88en+gRQCiAix1lqpS0CQAhDiMhne9gBjrf4aJiCV1kZbP+72h5fDYe/aJchuFSF9L2ATqLWwDRAQSCIQKOTlJ7IGWGvM1zABpZQ2xvlx2r98+/btYHrks8c9BfQgCLEJ1IItEAaShBDyTSpUSMXHgNn2zv/54/0eE9BGG+vHaX/49q8/XjQHPvXl797+jERYS8nYQ4F8/3lbDyLaMfjgJsCum7bOeeedH18O+2nia6/W8k3wZ8vw5JRiSikl7FYhPpbDENaScwwhpJwL4mOGxbt9a+OGYRjGYXo57HeDd1bzjv/mD9fKvyjFwCCtlVZaK/Xhk4mwlBzjuoQYcyl4VbXQ/TSgnXDauoFT3of9fhoHZ7SSUsp36+fsD+WwLsuyLgtYa421ID6phqJaS0ohrGtIOddH1YDmAWvjh5HzXtNuN3pntFTs3V2g+X+Y4zKf5tN8Et57jyA1cYXgmx/GmpsGpFQeVAAC2IZlu/sdDvtxHMfRW6OlFO9NAJCR4jqfXo/HVzmOuZLU+EkCGLGwBoQYc6n4mFHhXvWo+fb/8m0/+MEPg7O6ecAXP8xXgIo1x3U+vv748UPGgiC1xU8qAKg2DVhDyqXWh9SA8/3fOj/t9t/+OLT6L6M/yX10BzCFdT6+fv/rL5WRpLYFuUzwUgZEWLsGpJzLY5pACwGrTQP+ONjm/2j4EPrtHnDJcV1OP/768//rClJbzw7RO2A7BdYQ48PuAdBSQKrHf9j/0VrrjwcbAN8BS0lxnY+v3//8fwaktn4s+EkJCGHNOYWwpJTyowpAKsMR3/1+v5vGwXunlGLHtf/M+cGxlpJzznldYyoFqduP0nxqCrHdj6lirbWUkkupFfHjMfkf4W6ngDbOOuvs/tthN43e6u3G+x4EgLWwCziHVCpIba1rORP299tFAQgIaqm11lqx4vXrv5MABEht3eAHP+y+Hfbj4K1h9++D507Avm2KIYY4rzEjSG2sdc5Za63RuocKWtV0PQPxSkf4PgIQACCV9cM0jtPu5bDjGwD7fx/XTwBUc4phDesSUkGQyrquAbYZDgDvle8EcHXd/B0EIABACG3cOO12+/1+v59G79iU35tAK/TDWnIM67LMa8yVhObwqbPWGCWVUoKDJYRI+E4BHs4E+I7PGrB/Oex3u2kavDVayA+VD637hbDkFMMyzytrgOZSMWutNWqrGOF4Ida+CbAIHtAEQABIbdwwHV6+7cdpHEdvtfq09oOzAFhLiuu6nELkPaDvgcbafi1uGoBvNeDxTKDVv0gOgb38sR8Gz9n/jxmOCxPIOYZlPsWSSyWpadsEjOhFA0CEhLULADcJXPW899kDBAhl3DDtX84esPpJ3nPTgLDOp1wrIkgtWhLFGts+kvvG3ihArfXq9d9cAL3GRRtrnR/GceSzTJ7T+M21I+6FICSkZVnWdV3XUABAKQKYtpqJ7W21lFxyyfE0LytfhJEezRESPQzQ0pfW9cjl5U9RO9JZjWvF4/F0mpcQI0mppVRSTi/7aXBGXYTNMMcUU0zLXz9eT0tIGR/RBITkW7DpErDmQ/6qtQERlloYp+NxXtY1RKmVMsYYPe13o7dvBFBzXMO6hvmv76+nOaRyi/XfwQQ44W1a95ez+kP2hndzQsw5cy/cfGINSEoJbZ13fpymaXD6QgBQc1zneZlPP34c52WNuSJdL4KbmwDfYVTTAGfbHehD+KciVvb/Y0rzvLAGWBTKDuM4jsM4eGfkOw2Yj8fj8fV4PC0hlUrXn4J30QDFemy5Eo6bIt6aAGGtWCv7/zHEZVmWZQkxSRTKDtN+PzjvPjGB0+uPH6+n+TQvIeV6iwbS22+CXAXC5V/WWssVAG83QT7NSoohrCt/reu6hqhQaOunw8tgrDVWvxVAWObj97++r8u6LCH2aNFDmQBAW3/fAp1tbdFvN0GstdSSY1iWdVkWzgXEmCwJZYfd/tvA5VPv94DT648//woxhBBTrj19dg3uYALyoprXWfux8o9NoJacYliXeZ7n2OERtPXT4Q/fFOf9HvD6/c8/EyPXW3TR38MPkLL3Qiultl+wtf1TLZn3/6Uj5VxJKAPec8/U4PjaILacEcWwrsuyzKdTziVfnRDp+J1lchzRIcrc95bisizLuoaYqtDSICIe/vWyHwerm830ignEiutpnpc1xJzLTXzght8ogN78gzmGEEMIYQ3ruoYQkxCKg+X7b9/2k7dcMsn3KiyllFrKcjwtK9t+Kddfgzt+rwYgIVFNMazrui5rDDGEGFJWvYB+tz8cpsFq2VungWrJKeeUl+NpXtYYU/m6GsAtoDWnsM7LvCzNElKyWnHj/DRNu8k7fa4aJywpphjjzCaQcq5Y61fUgJb+Qj79TqfTzH/alLMW2m2dowNrQIssEJac1rCG0/HEG0ZGRES6kQR+swlUrP3qfzweS8mllJKLBWWH3X6/91w+dMGgAVhzDAtLbAkxptzu0Ld5qN8nAOrhjJJTWOfT6+vrObAByg67l2/fXCugP7eNUC0prstpPh67CXxZEpUmgWYCxx+vuEFoN+5e/vVfVspzEJTfxCYzH1+7CZRbOIAb7iCA5qB/cNSJai0llxjCui7zPB97KaBUmnPIB/OBR4lqSTEs85EPgZRLuenT3lgAvc9XtoLPirW7woSlpJRTDie++sa8VYu/7Dl79qZniENnteQU13Vu+l+uqwb4iJtrwBa75tRdRQGCBADTX8QY4zrPy7KGkJIRkm3+ZT8N3hr1dv1EQFA5abScTvMaYy7X1YN8xK0FQMCnfe0agCC6S1dzimtY100DhBLaOufc4bDj1AGce6bbHaDlTJb5tH4lDdjWXytXuQo2gbCuy3ya5yWEmJICqd0wjMN+P43emtYzJLZPIqJ2aZxPpxhCTDe6Ap1xjz0ANwlgRRQgJAhqJhDmeT7Ny7Ly7V9qO3D3wOBda5kRF59ExDWR6zLPMcWvoQEo32iAbN1uxD7wMh/ZBELiNOiwOxzGaWwx4HfBYySsJYWVb8HXVwR9xM33ACJCgeKsAQAgiUhsLs1p7hpQSWrnp8O3YfC+bYL9c/iTCLe4yamUWsrjawAQEVXYNAABBJOdYGkasCzzGkJMqZDQdpj237yzzll9KYBGIYfnPaCp1YMLgIhQCBBbDrMoABAsgVpKjiHGmHOuFUn0MkprjTF8BeJPAc4bYcWcYohhXZe1kao9tgAICAUKgFpKyTmnlDRqloBodBF5a/va76dx9M4Z0+reexIUoDJbRCmv319P8xrLlgl/bAEAESEIgFq7AIgABErYeKAKl39p7fa73TgMzlpzuX4CAqolctrk9S9OBNZ2/bnx+u/iB/DpVUvJqQtAEnXCkFwl9zyZdNhN0+C9NUZdNE4SEEHNKawhhPX4/ceRC2dueAW8wM01QBAhCGoakKMFEEIhQTOBXLCHzfN+v2MT6C0E0D1AwhzXZVmW+fj99bSEVBCvzoF8ilvvAYIABUlRa8kl55SSEFJWhWz41pVKqjW85MNumobBWaukklsGuTmAKSzH0+l0fH09LSHmeksq2Qvc/i4AAESCN8HEAlCIREIobYyrCJp73mze76dx8N4ataXPeuS45shBk+PpdJrXVJBuGgbYcPs9AEiAEJVtIKUkldKVCEBKZSwiCa1Nssm6st81E1CtsIITJ0RINcewnF6/fz8uy7yEmCtutSU3xT00gEC0UyDllJRSxSCRFEqbigTSGGNTdqnsuwmIcxxkcwATl00fwxrCmkq9g/oD3MUTBAACZD8gRqeUrqUiB34MCaGyscnmnMtumsbBO2ve9o8RIXYN+OvIkfN8aw+w414xQaw5rLNRskypVBRSUUUAqTQJqZTOueQ6jt7aizooEEit/m1dQwgxxphyLgVv7gBuuJcAqKa4WiWh5FIRpNKiIgmpCITUuphSSh0G78xFDYAAwFpLKbWsaxdBzqXe/AZwxt0EUHJclQAs/PfXVrIAhFS66FpqqdV7f1kJx6nQknPJOXQFSIUF8NU0AGuOWgCWXAmk0jYpJAIJUmHVfFVCZ509X4F4C2QHMq9rE0EqnAu903Pe0QRyFIQlZb4F+aSB6dAAWqAA0Rhzpk5hMWCtOcUU17A2FegVoXd60DsKQBCWFDJIZawPycg+RAAbTyy2HqIWNARoJtC6QkMIIaXEtMpfcA/g9esklLZuGJOTzJqjRA/3kVJSSfWGPbLHALdTILZM4FfTAKyAJSklszLWDWNIWQsJUmmleqkIdF7li+55bh0LYQ1riCHGmFp4/E7PeUcTwCKEAJG1dcO0xlSE5CvxJ7/yIhOINacUNw1I6U4P2HG/5CgBkBDE6eAUQ2Bn8G/+lAK44XKDvs8daMPdBECCvziuexbAp9vZBUkO19m1ihljTOlp1js95x3T4zwEpIVCY1hJSKV/4tN1CWyEY6rxbJjCJ8ZXFAC0+FDTAMdDBH5yoF1KQPWBI1obrRHFDUeKfMAd9wBBIEgQVu70dVIqU+rPzrNNAvLSBLTJWAVczR78c9zVBIAEYTcBJ5XJtv7Up+U19mrrZgLGmCoqvOeQuCHuaQIkeEBUMwFljP0V58tbCehOtiMACMV1HPq/wF33gPMmmGKw2tj8y4stib581VvtjTECAOV13MG/wt2LpAhrzSnFYI1N5ed7AMAbCZyFAITqSu7gX+F38Al2rqxfMMY2EA+dqJbckHKpCMLmlFJOSla6S2D89zBKSq6ef88a/9kPK6WRAEouHEgYYowxRK0y3cUj+F2Mkn39v2RPb0x5BCBqqQhCaTuENQRjlFQVsWKFmwzXOeM30Oltzp362/VDc5aFRCQOJMzrslitpJClVlFvNWRuw28xgdZJ9m+ZgJRaCCmJA2luGGbmnSdQpQgg+eBlcp/gPzEBkKSEEEoJkEpbP4TB8YwxpCzFtcMEPsFvMgHmVX3XP/jpT0sQQiFyJjWlOLb1F5QC6Fou+Y/4LYSKzbv9hELk0x8mIlRK2+xLzktbf64AhPVaBs0P+B0msLk2/4YEZGNVQW1LLbWWQUkgrDkVzpp8GQ1ow+KEtdY651wrBXrTCfj22/bGzsatVMUKKaVSSkHQSgrAWuRN08R3I1ISbXzuMAwjz1IbOBH4po/83WujYRQghCRJAMq4IeVKoKzREgjrTTpmN9xNA/q+N4zDMI7DOI7OOfO+j/yyr4CISLLc2jQaAABtnM9IILTRUhDWXAj5MHjoxkkhpFZaKz0MY5sjaK19J4CeH+iE6oRSKikBZFMFCUJb6yuBVEazBaSMEhEevmlKKqZTO09RNMYYo99RKWDz8Nu/SYVKAQgSJIQEAqGMQwKptOHDIBqNKADwwafOCil5ivDQB0kOF3mwBurcSNS/KaSm+wIESBKkjSOQWlsruNzamnrLINn9TEDxJLmhbwEjN1S/1wDsM0URK6IyAEKwdkveHQw7xd4S1hKDM1qIcruKwfuZgNTaWud4lOgwjmPb4N6xiVAvrUesWDUAXwS4b5CANIFQ2uZsseQYVmd0q8i9jQrcVwOcO5+C40cmKdqokbZ/+EZMxM1DBCBAaFNLLa7mFMLirAYgkreZOXtHAahWGj0O3jvrrDX9f52JBHpjeKm1MaRpa621yVrVk+kSQMiqK4pxHKd1HMeU8nYvejgCBYYAkFI3BdgmCJ3RC4JT59EppfKX6iyCnVlbc0gJCJSxfhh3qUCMKSYJhLdIG96LTk8oZaz1nsvg3m3+HTmmGFNMkXuISymqUchtULI5REIZ54cplQohBL4gEBGJKyVwNzY5qbV1bhjOQ7TOoHbq5xTCGkIIueSSc8lFWdvodwbvhwpSy6YCoLX1QyoVxWKUFIQl9/LxayRwFzY5AUIqZUzTgPcecGssQ2wsMuuSOxTP4TN2mKaMwPFBEBKlUMa6sVQE2X3ixD7xY5mA2K5yXQM+mkCnkqoprsu8zPOcUuaXaus3U8wVpHEEAnjEmDLWl4IglJZAtSStUODVOaN7aIDgEEDTgMHZN+1QnUmhYuujPx1PMaWcUkpJGcsDOHa5gjK+NI9AAqFxreSy+cRaCYSrfeLb7wGCLzJdA/om+IFNrdbKLJKvr68bl4a0TQIBQRmXKoEgwR6Rtrx+LQhLjlYrAKBr/aG7aIAQgo9B74fxbTUoADQmqcYkOx9ff3zvPEpJ8gBGYyNI44ZUEKB1X4Ph9VtDtaTojOb+gislcHs2OX5Za51nP8hZraU86ykRYiml5Lyu67LMp+MxxhhTjDEqa6xJ1mQ98eyAi25qbRBAKK1yimGxRisg+kjT+h/i1gJowT+13++maRqH1hP1Zg/A0kz+eDqd5mUNMeXKd15tmiPQ+DQvFUdIpTQSobWGh5MRXR8mv7UAZKts4XFq4zh4Z63Wl4yiVEuOMcYQXo+neVlD4FEpIFSbJGmta2+7kBvPJCMQ5Aw3GioivFYBbq8Bmh25XdeAwVxqQLsCNCql47E1kpfaaukbqbizTXHeaQD3IDbGcSklSbw2U3R7AfBQ3bMG6ItgMLcEYclpXZZ5OR5P87yuMVYkJBBSt1ma3nlnzVsN4GuiEJKsNcYopaTET6ia/0Pc3gSs88Pg9/vdNE7jMHieFiQBejS70aKcTqfTfJqXJYTIc6WkkMYY65z3vnUTijefrfibtUZrrZVCKa9OFd5YAFJp4/w4jvtNAzgndjYBIiw5rst8PM7zzGwaLRUgQXMWwXvvrH1LxiqEJBASFTSq3p5rfKw9QGrrhmHa7Xf9FGhjNWELBBAzCZ2Or/OyLMuyhih6tIhNwPvBO/s+giqEkkiEwpq+B3Cq6aFMgDVg2u33TQOcaDkigJYIoXYJOP5Yl3VlOmGl+AaljWEF4GPwjQMpJJAEAmGtYbLiT+YV/ce4vQbwXL3dbhrHYfDO9f9FvS8Ya27EMCGEmLeWOq216YySY4+jvPlwDpLpW60dAO7hCBnr/ThNg+fChvP/60TiOea2amVIautT2gbweT/4wQ/eTxMPpXqXRuGBbCnlzIG0xyNWltpYN7AAzBti6EaOWTGnXAoigNAgjSulFt0Lg/kEdN4xr/KbD2gpFEwxpcRxNLyeWPHmAlDGOj9O4/heAxqVbK0ppVIrt5Bpbodp6ze6zya2vjVVX3x2J9WIKbb185CN6574Dq6wdcMwTd47++4SiMylnmIbkyg0tExwH0rNfMzGGmOdd+6NCbAAa6kxppYy71NWrnniu+0BH3PBhLU2Wg12fdvNSTOfBP/Ta2R5Kte797MEY0w9lMwSuOqJ77UHjPb9Aoi7QlOOKedSEUCoFv5o0MborVOGt4UPGpRzjinllNucqcfbBNseMJnPFlBLTinwgDACoY33znvnuwjaSBkphfqQSeSe8pwyd1TnUgo+4iaoNZ8C6gOtfq8bjzH1Y9D6c+bcGmMMe3bbnArxiQnxFpBLLi2n+mACUNpY5wbP1JjiImjN3TM5bQqgjPXjbrfb7WwXwAWV1od/YQWIMYQYWQNaR+UjmQCWuM7H785O7NkYfR4bL7aWISGVNs6lcZpGdhj0J0V0nPLYikgqM9CHsPz143haQm7zFR4rL4AlhfnorV6dddY6kOfG6N40JjqjUOIh7H4bQ/KufAgICLCnznNY+bX8+HGc15gKtgL6q3APDXBGQfCDH5BTWyCIGyKVMlvtiEsptxnszuqNSGlDL5vCUguzMq0LE3Ivx+MFtdK1T3x7DVhnowTGKfHMLAK40IAW2jQ55ZyLs85Z56zpVZRvPmtjlGRKhbSR8c/zPPOMlX4/uAK3JlIqKcxKUIkxV5DaIrECkADJ/BmqaMN80rV5AdYoqX4yi5lbrlLKKSzzvMzLvMzruq5rTOUWFnBzDchxVQJLakPjBt70uBtIkhZSVb3xSXe3T8u3ZMrQd7+NUDHGGObTPM+nNo+CmYVu0FV8BxMQWFJIFaR2Q6647YJCgpQKkWrnk1atmFIxscLHFDo3k8cQ1hCW03w6nU6n09zmU9RGrvdgp0AUVNI6Z5DaDmNB2hqlhBQkN7JUrgvlSXqyFxa/+awLJu51Wdf5eDqejsfTcSm1llpLvUm16M0FIKikoE2R2vkpZSROXwuAj6f8RRPZh9hO41PiKdvrsszzK89Yel2wz+q6xRPfnEoLqxRAwBtXSskyTxrB2wqBd3NnoNnzBux/5tjG0S9L4xavN5mvtOEOvMJVCAC205RSkkoqAvhs3PKbt/G6O/icKKXEbQzTvCwh5lK3H77JE99eA1BUIGjrzykp1AB/V8TAfNwXA1V5DnPKOYWOdV26/3fDxrHbc4oiABBx5COnlDRtpe+/eBexw9vn7/U3p5RCLx7geSy3cYDPuL0JIABJ4pqnlFKmzp/0y/e1UZpbtVTigoHEl9/zt1xvc/xvuL0JsAfTlp9SIgAh/2bL5uFjpZbUEdsInpDyVk6ZS2ZubYKHnTBBglAQCtkMOKUEQsr6d+QxvWgknfW93f1SLrnthxxUxlu4P2fcfg/gMrncRugwqerfhS6Joz0lx23LW9Z1WZd1jbVdB7fGgtsSa95hwAIAAJTNBKRU6pesAdDLpkrKrXJiZUrdZVnm1EnKb+L4fsS9qsVryWFdjtZU773zORfT+APEZbNYi/tk3uFSCmtY2wyyhfOmubZeii9GoUE1xcVqCXEY/OCHlGzviBPn20CrG4fYN/0Q2xTOEMIaY+6h7zs9JdyTTS6FRSvAOIxDGFJKTrYqANFMGXGbxxMjzx6NMTGlOG+EKZeW+/l6LDJYctASao5jGGPKuSTJQ5SUqNgpFXvEM7Z4Jx96KXcniAWA14c+f447mkCQgCWFKYSYUi6eW+iVkheTw1vPXDvzwpr5yMulO0Tt6vP1yNSoJAmYIw9UzLnWzFlApWXhKGctvWEO145SKvOttl6aUm/s+X7A/QgVE2COwSwhch4vb/Gv7aZXt3GaC7+W0q2jthEtTCp/PwW4owYAFqWV9olzoViM5hy4bApecptCUuu6cL3UUtoYDdxedEu/9xPcTQMAOdjlUiqlIlJt2a+icm+P4I2g1rrM87zM87zUXkt48e2+uFvjJApAECBgbfOTy5YBbreblNskFqzs960h3I0++Ke4O58gYklBSwGFi2CaCZScS96aRte1l4v9dtybTU5wd48ATJo58pQqvUtu6xsOMabbD5H6t3BPQkUgAUQ16ygIS9SqsQm1g66WFgIkTDFyqOP3496EioC1JCCs2ahGKiGw733bVs+pwltP0fq3cH9CxVoEYc1Rb+NEe7983ZIk7Bjcjz76F7gzoaIAwkxYs9aqtxPBRhsBzc2l7hrf82F+grsRFW6/gBnFpTrTw5wD+/21cUjc+2k+eb77/4Zz4m8jVLwM7rRKmPs6vE888cQTTzzxxBNPPPHEE0888cQTTzzxxBNPPPHEE0888cQTTzzxxP9l/DctwRA/KeqiUgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7F2233BFFF90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "label:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJgho2AEBFbx"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class Classifier():\n",
        "    def __init__(self, sizes, epochs=10, l_rate=0.01):\n",
        "        self.sizes = sizes\n",
        "        self.epochs = epochs\n",
        "        self.l_rate = l_rate\n",
        "\n",
        "        # we save all parameters in the neural network in this dictionary\n",
        "        self.params = self.initialization()\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "        if derivative:\n",
        "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "        return 1/(1+np.exp(-1*x))\n",
        "\n",
        "    def softmax(self, x, derivative=False):\n",
        "        # Numerically stable with large exponentials\n",
        "        exps = np.exp(x - x.max())\n",
        "        if derivative:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return np.exp(x)/(np.sum(np.exp(x),axis=0))\n",
        "\n",
        "    def initialization(self):\n",
        "        # number of nodes in each layer\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def forward_pass(self, x_train):\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['A0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        params['Z1'] = np.dot(params['W1'], params['A0']) \n",
        "        params['A1'] = self.sigmoid(params['Z1'])\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['Z2'] = np.dot(params['W2'], params['A1'])\n",
        "        params['A2'] = self.sigmoid(params['Z2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['Z3'] = np.dot(params['W3'], params['A2'])\n",
        "        params['A3'] = self.softmax(params['Z3'])\n",
        "\n",
        "        return params['A3']\n",
        "\n",
        "    def backward_pass(self, y_train, output):\n",
        "        '''\n",
        "            This is the backpropagation algorithm, for calculating the updates\n",
        "            of the neural network's parameters.\n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
        "        change_w['W3'] = np.outer(error, params['A2'])\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = np.dot(params['W3'].T, error)*self.sigmoid(params['Z2'], derivative=True)\n",
        "        change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error)*self.sigmoid(params['Z1'], derivative=True)\n",
        "        change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w):\n",
        "        '''\n",
        "            Update network parameters according to update rule from\n",
        "            Stochastic Gradient Descent.\n",
        "\n",
        "            θ = θ - η * ∇J(x, y), \n",
        "                theta θ:            a network parameter (e.g. a weight w)\n",
        "                eta η:              the learning rate\n",
        "                gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                    i.e. the change for a specific theta θ\n",
        "        '''\n",
        "        \n",
        "        for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate*value\n",
        "\n",
        "    def compute_accuracy(self, x_val, y_val):\n",
        "        '''\n",
        "            This function does a forward pass of x, then checks if the indices\n",
        "            of the maximum value in the output equals the indices in the label\n",
        "            y. Then it sums over each prediction and calculates the accuracy.\n",
        "        '''\n",
        "        predictions = []\n",
        "\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = self.forward_pass(x)\n",
        "            pred = np.argmax(output)\n",
        "            predictions.append(pred == np.argmax(y))\n",
        "        \n",
        "        return np.mean(predictions)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val):\n",
        "        '''\n",
        "            complete this function to train the model. Calculate predictions,\n",
        "            calculate changes to w, update parameters in the training loop.\n",
        "            Wee are using Stocastic Gradient Descent here\n",
        "        '''\n",
        "        start_time = time.time()\n",
        "        for iteration in range(self.epochs):\n",
        "            for x,y in zip(x_train, y_train):\n",
        "                # YOUR CODE HERE\n",
        "                a3 = self.forward_pass(x)\n",
        "                grad_w = self.backward_pass(y, a3)\n",
        "                self.update_network_parameters(grad_w)\n",
        "\n",
        "                pass\n",
        "                # YOUR CODE HERE\n",
        "            accuracy = self.compute_accuracy(x_val, y_val)\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                iteration+1, time.time() - start_time, accuracy * 100\n",
        "            ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLc4Bay65TyA",
        "outputId": "a19e42aa-af47-4462-93f9-3234b0ec09bd"
      },
      "source": [
        "dnn = Classifier(sizes=[784, 128, 128, 10],l_rate=0.1,epochs=5)\n",
        "dnn.train(x_train, y_train, x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Time Spent: 58.62s, Accuracy: 89.30%\n",
            "Epoch: 2, Time Spent: 118.32s, Accuracy: 91.78%\n",
            "Epoch: 3, Time Spent: 178.44s, Accuracy: 92.94%\n",
            "Epoch: 4, Time Spent: 237.78s, Accuracy: 93.71%\n",
            "Epoch: 5, Time Spent: 297.16s, Accuracy: 94.38%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0wasTe3UEZR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}